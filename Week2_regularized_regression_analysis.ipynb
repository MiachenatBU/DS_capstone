{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "# Regularized Regression Analysis of CVD Healthcare Costs\n",
                "    "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "This notebook applies regularized regression models (Lasso, Ridge, and Elastic Net) to the data. These models are particularly useful for handling multicollinearity and performing feature selection, which are important considerations for this dataset. This analysis is a follow-up to the initial linear regression."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "## 1. Data Loading and Preparation\n",
                "We start by loading the dataset and preparing it for analysis. This includes cleaning the data and scaling the features, which is a crucial step for regularized regression models.\n",
                "    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from sklearn.linear_model import Lasso, Ridge, ElasticNet\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "    \n",
                "# Load the dataset\n",
                "df = pd.read_csv('CVD_data2.csv')\n",
                "    \n",
                "# Select and clean the relevant variables\n",
                "regression_vars = ['TOTTCHY1', 'TOTTCHY1_rank', 'total_comorbidities', 'AGEY1X', 'PRVEVY1']\n",
                "df_clean = df[regression_vars].dropna()\n",
                "    \n",
                "# Define independent and dependent variables\n",
                "X = df_clean[['total_comorbidities', 'AGEY1X', 'PRVEVY1']]\n",
                "y_raw = df_clean['TOTTCHY1']\n",
                "y_rank = df_clean['TOTTCHY1_rank']\n",
                "    \n",
                "# Scale the independent variables\n",
                "scaler = StandardScaler()\n",
                "X_scaled = scaler.fit_transform(X)\n",
                "    "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "## 2. Lasso Regression\n",
                "Lasso (Least Absolute Shrinkage and Selection Operator) regression can be used for feature selection as it can shrink the coefficients of less important features to zero.\n",
                "    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Lasso Coefficients (Raw Costs): [9792.57325841 4498.16546045  369.48427867]\n",
                        "Lasso Coefficients (Ranked Costs): [ 525.66367354  469.97952929 -162.0848049 ]\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "# Split data for raw cost prediction\n",
                "X_train, X_test, y_train_raw, y_test_raw = train_test_split(X_scaled, y_raw, test_size=0.2, random_state=42)\n",
                "    \n",
                "# Fit Lasso model on raw costs\n",
                "lasso_raw = Lasso(alpha=1.0)\n",
                "lasso_raw.fit(X_train, y_train_raw)\n",
                "print(\"Lasso Coefficients (Raw Costs):\", lasso_raw.coef_)\n",
                "    \n",
                "# Split data for ranked cost prediction\n",
                "X_train, X_test, y_train_rank, y_test_rank = train_test_split(X_scaled, y_rank, test_size=0.2, random_state=42)\n",
                "    \n",
                "# Fit Lasso model on ranked costs\n",
                "lasso_rank = Lasso(alpha=1.0)\n",
                "lasso_rank.fit(X_train, y_train_rank)\n",
                "print(\"Lasso Coefficients (Ranked Costs):\", lasso_rank.coef_)\n",
                "    "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "## 3. Ridge Regression\n",
                "Ridge regression is useful for mitigating the impact of multicollinearity by shrinking the coefficients of correlated predictors.\n",
                "    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Ridge Coefficients (Raw Costs): [9790.94945895 4499.02388136  370.61128528]\n",
                        "Ridge Coefficients (Ranked Costs): [ 526.44046452  470.60056032 -163.24934298]\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "# Fit Ridge model on raw costs\n",
                "ridge_raw = Ridge(alpha=1.0)\n",
                "ridge_raw.fit(X_train, y_train_raw)\n",
                "print(\"Ridge Coefficients (Raw Costs):\", ridge_raw.coef_)\n",
                "    \n",
                "# Fit Ridge model on ranked costs\n",
                "ridge_rank = Ridge(alpha=1.0)\n",
                "ridge_rank.fit(X_train, y_train_rank)\n",
                "print(\"Ridge Coefficients (Ranked Costs):\", ridge_rank.coef_)\n",
                "    "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "## 4. Elastic Net Regression\n",
                "Elastic Net is a hybrid of Lasso and Ridge regression, incorporating the benefits of both. It can perform feature selection while handling multicollinearity.\n",
                "    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Elastic Net Coefficients (Raw Costs): [6657.3176066  4057.90028358  639.23582512]\n",
                        "Elastic Net Coefficients (Ranked Costs): [380.17369617 359.03082249 -84.26335847]\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "# Fit Elastic Net model on raw costs\n",
                "elastic_net_raw = ElasticNet(alpha=1.0, l1_ratio=0.5)\n",
                "elastic_net_raw.fit(X_train, y_train_raw)\n",
                "print(\"Elastic Net Coefficients (Raw Costs):\", elastic_net_raw.coef_)\n",
                "    \n",
                "# Fit Elastic Net model on ranked costs\n",
                "elastic_net_rank = ElasticNet(alpha=1.0, l1_ratio=0.5)\n",
                "elastic_net_rank.fit(X_train, y_train_rank)\n",
                "print(\"Elastic Net Coefficients (Ranked Costs):\", elastic_net_rank.coef_)\n",
                "    "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "## 5. Conclusion\n",
                "This notebook demonstrated the application of Lasso, Ridge, and Elastic Net regression models for analyzing healthcare costs. Key observations include:\n",
                "- The models, especially when applied to the ranked cost data, provide insights into the relative importance of the predictors.\n",
                "- The number of comorbidities and age consistently show a positive relationship with healthcare costs across all models, while private insurance status shows a negative relationship in the ranked models.\n",
                "- The coefficients from these regularized models can be used to select the most influential features and to build more robust predictive models, as proposed in the project document.\n",
                "\n",
                "    "
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
